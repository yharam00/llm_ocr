import streamlit as st
import requests
import json
from openai import OpenAI
import google.generativeai as genai
from typing import Dict, Any, Optional
import io

# --- Constants & Default Values ---

# Default system prompt
DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant. Keep your responses concise."

# Default user prompt template (leave empty if not needed)
DEFAULT_USER_TEMPLATE = ""

# --- Session State Initialization ---
# Initialize session state keys to ensure they exist on first run.
# This preserves values across app reruns (e.g., when the 'Send' button is clicked).

def init_session_state():
    """Initializes all necessary keys in Streamlit's session state."""
    defaults = {
        "provider": "OpenAI",
        "openai_api_key": "",
        "openai_model_name": "gpt-4o-mini",
        "google_api_key": "",
        "gemini_model_name": "gemini-2.5-pro",
        "ollama_base_url": "http://localhost:11434",
        "ollama_model_name": "llama3",
        "system_prompt": DEFAULT_SYSTEM_PROMPT,
        "user_template": DEFAULT_USER_TEMPLATE,
        "user_input": "",
        "last_response": "",
        "last_provider_info": "",
        "pdf_extracted_text": "" # PDF ì¶”ì¶œ ê²°ê³¼ë¥¼ ì €ì¥í•  í‚¤
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

# --- API Call Logic ---

def call_openai(api_key: str, model: str, system_prompt: str, combined_user_prompt: str) -> str:
    """
    Calls the OpenAI Chat Completions API.
    """
    try:
        client = OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": combined_user_prompt},
            ],
        )
        answer = response.choices[0].message.content
        return answer
    except Exception as e:
        st.error(f"OpenAI API Error: {e}")
        return ""

def call_gemini(api_key: str, model: str, system_prompt: str, combined_user_prompt: str) -> str:
    """
    Calls the Google Gemini (Generative AI) API.
    """
    try:
        genai.configure(api_key=api_key)
        
        # System prompt handling for Gemini
        generation_config = {}
        safety_settings = {} # Add safety settings if needed
        
        model_instance = genai.GenerativeModel(
            model_name=model,
            system_instruction=system_prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        
        # Start a chat session to maintain context (though here we only send one message)
        # For a simple, non-chat use case, you can also use generate_content
        response = model_instance.generate_content(combined_user_prompt)
        
        return response.text
    except Exception as e:
        st.error(f"Google Gemini API Error: {e}")
        return ""

def call_ollama(base_url: str, model: str, system_prompt: str, combined_user_prompt: str) -> str:
    """
    Calls a self-hosted Ollama API (assuming OpenAI-compatible /v1/chat/completions endpoint).
    """
    try:
        # Construct the API endpoint
        api_url = f"{base_url.rstrip('/')}/v1/chat/completions"
        
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": combined_user_prompt},
            ],
            "stream": False,  # As requested
        }
        
        response = requests.post(
            api_url, 
            json=payload, 
            timeout=60,
            headers={"Content-Type": "application/json"}
        )
        
        # Check for HTTP errors
        response.raise_for_status()
        
        response_json = response.json()
        answer = response_json["choices"][0]["message"]["content"]
        return answer
    except requests.exceptions.ConnectionError:
        st.error(f"Ollama Connection Error: Could not connect to {api_url}. Is the server running?")
        return ""
    except requests.exceptions.Timeout:
        st.error("Ollama Error: Request timed out.")
        return ""
    except Exception as e:
        st.error(f"Ollama API Error: {e}")
        return ""

def call_gemini_for_pdf_extraction(api_key: str, pdf_bytes: bytes) -> str:
    """
    Calls the Google Gemini API (gemini-2.5-pro) to extract text from a PDF.
    """
    try:
        genai.configure(api_key=api_key)
        
        # Use a model that supports file inputs, like gemini-1.5-pro
        model_instance = genai.GenerativeModel(model_name="gemini-2.5-pro")
        
        # Create the PDF part for the prompt
        pdf_part = {"mime_type": "application/pdf", "data": pdf_bytes}
        
        # Define the prompt for text extraction
        prompt = """
**System / Instruction (ì—­í•  ì§€ì •)**
ë‹¹ì‹ ì€ PDF ë¬¸ì„œ(ìŠ¤ìº”/í…ìŠ¤íŠ¸ í˜¼í•© í¬í•¨)ì—ì„œ **ì •í™•í•œ OCR**ì„ ìˆ˜í–‰í•˜ëŠ” ì „ë¬¸ ë¶„ì„ê°€ì´ë‹¤.
ì•„ë˜ ê·œì¹™ì„ ì² ì €íˆ ì§€ì¼œ **ì§€ì •ëœ ì¶œë ¥ í˜•ì‹ìœ¼ë¡œë§Œ** ë‚´ë³´ë‚¸ë‹¤. ì¶”ì¸¡/ë³´ì •/ìš”ì•½ ê¸ˆì§€. ì›ë¬¸ ì¶©ì‹¤ ì¬í˜„.

**í•µì‹¬ ê·œì¹™**

1. **íŒë… ë¶ˆê°€/ëˆ„ë½**ì€ ìƒì„±í•˜ì§€ ë§ê³  `âŸ¨UNREADABLEâŸ©`ë¡œ í‘œê¸°.
2. **ì›ë¬¸ ìˆœì„œ ë³´ì¡´:** ì§€ë©´ ìƒ ì¢Œâ†’ìš°, ìƒâ†’í•˜, **ë‹¤ë‹¨(2-column) ìš°ì„  ê·œì¹™** ì¤€ìˆ˜(1ì—´ ì „ì²´â†’2ì—´ ì „ì²´). ë™ì‹œ ë°°ì¹˜ í…ìŠ¤íŠ¸ëŠ” ì¢Œí‘œ/ë¸”ë¡ ìˆœì„œë¡œ ì •ë ¬.
3. **ì¤„ë°”ê¿ˆ/í•˜ì´í”ˆ ì²˜ë¦¬:** ì¤„ ë í•˜ì´í”ˆì€ ë‹¨ì–´ ì—°ê²°(`hyphenation fix`), ë¬¸ë‹¨ ë‚´ ê°•ì œ ì¤„ë°”ê¿ˆ ì œê±°(ë‹¨, ì‹œ/ì½”ë“œ/ì£¼ì†Œ ë“±ì€ ìœ ì§€).
4. **íŠ¹ìˆ˜ê¸°í˜¸Â·ìˆ˜ì‹Â·ë¬¸ìì…‹**: ì†ì‹¤ ì—†ì´ ë³µì›. ìˆ˜ì‹ì€ LaTeXë¡œ ê°ì‹¸ê¸°(`$â€¦$` or `$$â€¦$$`).
5. **í‘œ(Table)**: ê° í‘œë¥¼ **TSV**ì™€ **Markdown í‘œ** ë‘ í˜•íƒœë¡œ ë™ì‹œì— ì œê³µ. ë³‘í•©ì…€ì€ ë¹ˆì¹¸ ìœ ì§€ + `rowspan/colspan` ë©”ëª¨.
6. **ì²´í¬ë°•ìŠ¤/ë¼ë””ì˜¤/ë„í˜•**: `â˜‘/â˜`, `â—/â—‹` ë“±ìœ¼ë¡œ ëª…ì‹œ. ì„ íƒ í•´ì„ ê¸ˆì§€.
7. **ê°ì£¼/ë¯¸ì£¼/ì£¼ì„**: ë³¸ë¬¸ ìœ„ì¹˜ì— ê°ì£¼ í‘œì‹ ìœ ì§€, í•˜ë‹¨ì— `[Footnote n] â€¦`ë¡œ ëª¨ì•„ ì ê¸°.
8. **ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§**: í˜ì´ì§€ ë²ˆí˜¸/ë¬¸ì„œëª…ì€ ë³¸ë¬¸ê³¼ ë¶„ë¦¬í•´ `[Header]`, `[Footer]` ë¸”ë¡ìœ¼ë¡œ.
9. **ì´ë¯¸ì§€/ë„í‘œ/ìŠ¤íƒ¬í”„**: ë‚´ìš© í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ OCR, ì—†ìœ¼ë©´ **ALT ì„¤ëª…**ìœ¼ë¡œ `![ALT: â€¦]` 1ì¤„.
10. **ì–¸ì–´/ì½”ë“œ í˜¼ìš©**: ì›ì–´ ìœ ì§€. ë‚ ì§œÂ·ìˆ«ì **í˜•íƒœë¥¼ ë°”ê¾¸ì§€ ë§ ê²ƒ**(ì •ê·œí™” ê¸ˆì§€).
11. **ë³´ì•ˆ/ë¯¼ê°ì •ë³´**: ë§ˆìŠ¤í‚¹ í”ì ì€ ê·¸ëŒ€ë¡œ í‘œê¸°(ì˜ˆ: `****`). ì„ì˜ ë³µì› ê¸ˆì§€.
12. **ë¬´ì†ì‹¤ ì›ì¹™**: í•´ì„/ìš”ì•½/ì¹˜í™˜ í•˜ì§€ ë§ê³  **ë³´ì´ëŠ” ê·¸ëŒ€ë¡œ** ì „ì‚¬.

**ì¶œë ¥ í˜•ì‹ (ì´ ìˆœì„œë¡œë§Œ)**

```
=== OCR-METADATA ===
file_name: {{íŒŒì¼ëª…}}
page_count: {{ì •ìˆ˜}}
mode: {{"auto" | "text-priority" | "layout-priority"}}
notes: [[ì„ íšŒ/ì™œê³¡/ì €í•´ìƒë„/ì›Œí„°ë§ˆí¬ ë“± ê°ì§€ëœ ì´ìŠˆ ê°„ë‹¨ ê¸°ì¬]]

=== PAGES ===
----- PAGE 1 -----
[Header]
{ì¡´ì¬í•˜ë©´ í—¤ë” í…ìŠ¤íŠ¸ 1~3ì¤„}
[/Header]

[Body]
{ë³¸ë¬¸ í…ìŠ¤íŠ¸. ë‹¨ë½ì€ ë¹ˆì¤„ 1ê°œë¡œ êµ¬ë¶„. ë‹¤ë‹¨ì€ 1ì—´ ì „ì²´â†’2ì—´ ì „ì²´ ìˆœì„œ}
- í‘œëŠ” ì•„ë˜ ê·œì¹™ìœ¼ë¡œ ì‚½ì…:
Â  [Table 1 - Markdown]
Â  | Col1 | Col2 | ...
Â  |------|------|---
Â  | ...Â  | ...Â  |
Â  [/Table 1]
Â  [Table 1 - TSV]
Â  Col1\tCol2\t...
Â  ...\t...\t...
Â  [/Table 1]
- ìˆ˜ì‹: $E=mc^2$
- ì²´í¬ë°•ìŠ¤ ì˜ˆ: â˜‘ ë™ì˜í•¨ / â˜ ë¹„ë™ì˜
- ê·¸ë¦¼/ë„í‘œ: ![ALT: ë°” ì°¨íŠ¸(ë²”ë¡€: A/B/C), ê°’ í…ìŠ¤íŠ¸ ì—†ìŒ]
[/Body]

[Footnotes]
[Footnote 1] â€¦
[Footnote 2] â€¦
[/Footnotes]

[Footer]
{ì¡´ì¬í•˜ë©´ í‘¸í„° í…ìŠ¤íŠ¸ 1~3ì¤„}
[/Footer]
----- PAGE 1 END -----

----- PAGE 2 -----
{ë™ì¼ í¬ë§· ë°˜ë³µ}
----- PAGE 2 END -----
```

**í’ˆì§ˆ ì œì–´(ëª¨ë¸ ë‚´ë¶€ í–‰ë™ ì§€ì‹œ)**

* í˜ì´ì§€ë³„ **íšŒì „(0/90/180/270)** ìë™ ê°ì§€ í›„ ì¬ë°°ì¹˜.
* í‘œëŠ” ì„ /ê²©ì ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ ì…€ ê²½ê³„ ì¶”ì •. ìˆ«ìì—´ì€ ìˆ«ìë¡œ ë³´ì¡´(ì‰¼í‘œ/ë‹¨ìœ„ ìœ ì§€, ë³€í™˜ ê¸ˆì§€).
* **ì¢Œí‘œ ê¸°ë°˜ ë¸”ë¡ ë³‘í•©**ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìˆœì„œ í™•ì •(ë„í˜•/ìº¡ì…˜ì€ ë³¸ë¬¸ ì§í›„).
* **ë¼í‹´/í•œê¸€/ê¸°í˜¸** ì•ˆì • ì¸ì‹(ffi/fi í•©ì, â€˜â€“â€™ vs â€˜â€”â€™ êµ¬ë¶„).
* **ë‚®ì€ ì‹ ë¢°ë„** í† í°ì€ `âŸ¨?âŸ©`ë¡œ ë‘˜ëŸ¬ í‘œê¸°(ì˜ˆ: `ê°œâŸ¨?âŸ©ë°œ`).
* PDFì— **ë‚´ì¥ í…ìŠ¤íŠ¸**ê°€ ìˆìœ¼ë©´ ìš°ì„  ì¶”ì¶œí•˜ë˜, ì†ì‹¤/ê¹¨ì§ì´ ë³´ì´ë©´ í•´ë‹¹ ë¸”ë¡ë§Œ ì´ë¯¸ì§€ OCRë¡œ ëŒ€ì²´ í›„ ë³‘í•©.

**ëª¨ë“œ ìŠ¤ìœ„ì¹˜(ì„ íƒ, ê¸°ë³¸: auto)**

* `text-priority`: í‘œ ë‹¨ìˆœí™”, ê¸€ì ê°€ë…ì„± ìš°ì„ .
* `layout-priority`: ë ˆì´ì•„ì›ƒ ë³´ì¡´(ìº¡ì…˜/ë°•ìŠ¤/ì‚¬ì´ë“œë°”ë¥¼ Body ë‚´ ë¸”ë¡ìœ¼ë¡œ ìœ ì§€).

**ì…ë ¥**

* ì²¨ë¶€: `{{PDF íŒŒì¼}}` (ë˜ëŠ” í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ë°°ì—´)
* ì„ íƒ ë§¤ê°œë³€ìˆ˜:

Â  * `pages`: ì˜ˆ) `1-3,5,7-8`
Â  * `detect_language`: true
Â  * `output_tables`: true
Â  * `max_alt_length`: 30 (ì´ë¯¸ì§€ ALT ìµœëŒ€ ê¸€ììˆ˜)

**ì¶œë ¥ ì œí•œ**

* ìœ„ì˜ **ì¶œë ¥ í˜•ì‹ ë¸”ë¡**ë§Œ ì¶œë ¥. ë‹¤ë¥¸ ì„¤ëª…Â·ì‚¬ê³¼Â·ì¶”ê°€ ì½”ë©˜íŠ¸ ê¸ˆì§€.

---

## ğŸ”§ ê°„ë‹¨ ì‚¬ìš© ì˜ˆ

* **í”„ë¡¬í”„íŠ¸**: ìœ„ â€œë²”ìš© í”„ë¡¬í”„íŠ¸â€ ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê³ 

Â  * ì²¨ë¶€: `report.pdf`
Â  * ì˜µì…˜: `pages=1-5`, `mode=layout-priority`, `output_tables=true`

* **ëª¨ë¸ ì‘ë‹µ(ìš”ì•½ ì˜ˆì‹œ, ì¼ë¶€)**

```
=== OCR-METADATA ===
file_name: report.pdf
page_count: 12
mode: layout-priority
notes: [slight skew on page 2; watermarked background detected]

=== PAGES ===
----- PAGE 1 -----
[Header]
ACME Corp â€” Annual Summary (Confidential)
[/Header]

[Body]
ì„œë¡ 
ë³¸ ë³´ê³ ì„œëŠ” â€¦

[Table 1 - Markdown]
| í•­ëª© | ê°’ | ë‹¨ìœ„ |
|-----|----|------|
| ê¸¸ì´ | 12.3 | cm |
[/Table 1]
[Table 1 - TSV]
í•­ëª©\tê°’\të‹¨ìœ„
ê¸¸ì´\t12.3\tcm
[/Table 1]

ê·¸ë¦¼ 1. ![ALT: ì„ ê·¸ë˜í”„(2019â€“2024, 6ê°œ ì , ë²”ë¡€ ì—†ìŒ)]
[/Body]

[Footnotes]
[Footnote 1] ìë£Œ ì¶œì²˜: ë‚´ë¶€ DB
[/Footnotes]

[Footer]
Page 1 of 12
[/Footer]
----- PAGE 1 END -----
```
        """
        
        # Generate content
        response = model_instance.generate_content([prompt, pdf_part])
        
        return response.text
    except Exception as e:
        st.error(f"Gemini PDF ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

# --- Helper Functions ---

def validate_inputs() -> bool:
    """
    Checks if the necessary API keys or configs for the selected provider are present.
    """
    provider = st.session_state.provider
    
    if provider == "OpenAI":
        if not st.session_state.openai_api_key:
            st.error("Please enter your OpenAI API Key in the settings.")
            return False
        if not st.session_state.openai_model_name:
            st.error("Please enter an OpenAI model name in the settings.")
            return False
            
    elif provider == "Google Gemini":
        if not st.session_state.google_api_key:
            st.error("Please enter your Google Gemini API Key in the settings.")
            return False
        if not st.session_state.gemini_model_name:
            st.error("Please enter a Gemini model name in the settings.")
            return False

    elif provider == "Ollama (self-hosted)":
        if not st.session_state.ollama_base_url:
            st.error("Please enter the Ollama Base URL in the settings.")
            return False
        if not st.session_state.ollama_model_name:
            st.error("Please enter an Ollama model name in the settings.")
            return False
            
    if not st.session_state.user_input:
        st.warning("Please enter some text in the user input field.")
        return False
        
    return True

def get_llm_response():
    """
    Validates inputs and calls the appropriate LLM provider function.
    Updates session state with the response.
    """
    if not validate_inputs():
        return

    # Combine the user prompt template with the main user input
    # This is where the prompt components are assembled.
    combined_user_prompt = f"{st.session_state.user_template}\n\n{st.session_state.user_input}".strip()

    provider = st.session_state.provider
    response = ""
    provider_info = ""

    with st.spinner(f"Waiting for {provider} response..."):
        try:
            if provider == "OpenAI":
                provider_info = f"Provider: OpenAI | Model: {st.session_state.openai_model_name}"
                response = call_openai(
                    api_key=st.session_state.openai_api_key,
                    model=st.session_state.openai_model_name,
                    system_prompt=st.session_state.system_prompt,
                    combined_user_prompt=combined_user_prompt
                )
            elif provider == "Google Gemini":
                provider_info = f"Provider: Google Gemini | Model: {st.session_state.gemini_model_name}"
                response = call_gemini(
                    api_key=st.session_state.google_api_key,
                    model=st.session_state.gemini_model_name,
                    system_prompt=st.session_state.system_prompt,
                    combined_user_prompt=combined_user_prompt
                )
            elif provider == "Ollama (self-hosted)":
                provider_info = f"Provider: Ollama | Model: {st.session_state.ollama_model_name}"
                response = call_ollama(
                    base_url=st.session_state.ollama_base_url,
                    model=st.session_state.ollama_model_name,
                    system_prompt=st.session_state.system_prompt,
                    combined_user_prompt=combined_user_prompt
                )
        except Exception as e:
            # This is a fallback catch-all, though specific errors are handled in provider functions.
            st.error(f"An unexpected error occurred: {e}")
    
    # Store the response in session state so it persists
    st.session_state.last_response = response
    st.session_state.last_provider_info = provider_info

# --- PDF Extraction Logic ---
def handle_pdf_extraction():
    """
    Handles the logic for PDF extraction when the button is clicked.
    """
    uploaded_file = st.session_state.get("pdf_file")
    
    if not uploaded_file:
        st.warning("PDF íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    if not st.session_state.google_api_key:
        st.error("PDF ì¶”ì¶œì„ ìœ„í•´ Google Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°” 'Settings'ì—ì„œ í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ ì¤‘... (Gemini API ì‚¬ìš©)"):
        pdf_bytes = uploaded_file.getvalue()
        extracted_text = call_gemini_for_pdf_extraction(
            api_key=st.session_state.google_api_key,
            pdf_bytes=pdf_bytes
        )
        st.session_state.pdf_extracted_text = extracted_text

# --- Streamlit UI ---

def build_ui():
    """Constructs the Streamlit UI components."""
    
    # --- Settings Sidebar ---
    # All settings are placed in the sidebar to keep the main UI clean.
    with st.sidebar:
        st.title("Settings")
        st.write("Configure your LLM provider and prompts here.")

        # --- Model Provider Selection ---
        st.selectbox(
            "Select LLM Provider",
            options=["OpenAI", "Google Gemini", "Ollama (self-hosted)"],
            key="provider"
        )
        
        st.divider()

        # --- Provider-Specific Settings ---
        provider = st.session_state.provider
        
        if provider == "OpenAI":
            st.subheader("OpenAI Settings")
            st.text_input(
                "OpenAI API Key",
                type="password",
                key="openai_api_key",
                help="Get your key from https://platform.openai.com/api-keys"
            )
            st.text_input(
                "Model Name",
                key="openai_model_name",
                help="E.g., gpt-4o-mini, gpt-4-turbo"
            )

        elif provider == "Google Gemini":
            st.subheader("Google Gemini Settings")
            st.text_input(
                "Google Gemini API Key",
                type="password",
                key="google_api_key",
                help="Get your key from https://aistudio.google.com/app/api-keys"
            )
            st.text_input(
                "Model Name",
                key="gemini_model_name",
                help="E.g., gemini-2.5-pro"
            )

        elif provider == "Ollama (self-hosted)":
            st.subheader("Ollama (self-hosted) Settings")
            st.text_input(
                "Ollama Base URL",
                key="ollama_base_url",
                help="E.g., http://localhost:11434"
            )
            st.text_input(
                "Model Name",
                key="ollama_model_name",
                help="E.g., llama3, phi3 (must be compatible with OpenAI API endpoint)"
            )
            st.caption("Note: Assumes Ollama is serving an OpenAI-compatible API at `/v1/chat/completions`.")

        st.divider()

        # --- Prompt Configuration ---
        st.subheader("Prompt Configuration")
        st.text_area(
            "System Prompt",
            key="system_prompt",
            height=150,
            help="The system-level instructions for the LLM."
        )
        st.text_area(
            "User Prompt Template",
            key="user_template",
            height=100,
            help="Optional text to prepend to your main input. (e.g., 'Summarize the following text: ')"
        )

    # --- Main UI ---
    st.title("LLM Playground")

    # --- Tabs for different modes ---
    tab1, tab2 = st.tabs(["LLM Playground", "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"])

    # --- Tab 1: LLM Playground (Original) ---
    with tab1:
        # Main user input text area
        st.text_area(
            "User Input",
            key="user_input",
            height=300,
            placeholder="Enter your prompt here..."
        )

        # Send button
        st.button(
            "Send",
            on_click=get_llm_response, # Function to call when clicked
            type="primary",
            key="send_button"
        )

        # --- Response Area ---
        if st.session_state.last_response:
            st.markdown("---")
            st.info(st.session_state.last_provider_info)
            # Use st.container with a border for a nicely formatted box
            with st.container(border=True):
                st.markdown(st.session_state.last_response)
    
    # --- Tab 2: PDF Text Extraction ---
    with tab2:
        st.header("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (Gemini API)")
        st.info(
            "ì´ ê¸°ëŠ¥ì€ Google Gemini API ('gemini-2.5-pro')ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. "
            "ì‚¬ì´ë“œë°” 'Settings'ì—ì„œ Google Gemini API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        )

        # PDF file uploader
        st.file_uploader(
            "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=["pdf"],
            key="pdf_file"
        )

        # Extraction button
        st.button(
            "í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ê¸°",
            on_click=handle_pdf_extraction,
            key="pdf_extract_button"
        )

        # --- PDF Extraction Response Area ---
        if st.session_state.pdf_extracted_text:
            st.markdown("---")
            st.subheader("ì¶”ì¶œëœ í…ìŠ¤íŠ¸")
            with st.container(border=True, height=500):
                st.markdown(st.session_state.pdf_extracted_text)


# --- App Entry Point ---

def main():
    st.set_page_config(
        page_title="LLM Playground",
        page_icon="ğŸ¤–",
        layout="centered"
    )
    
    # 1. Initialize session state
    init_session_state()
    
    # 2. Build the UI
    build_ui()

if __name__ == "__main__":
    main()